# -*- coding: utf-8 -*-
"""Fetal_HC_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8DxmNlBCJ5hLte9NCivymofapHBwBsO
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pylab as plt

import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.applications import ResNet50, VGG16
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dropout, Dense
from tensorflow.keras.preprocessing.image import img_to_array, load_img
from tqdm import tqdm
from keras import regularizers
from torchvision import transforms

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

from google.colab import drive
drive.mount('/content/drive')

image_transforms = {
    'train': transforms.Compose([
        transforms.Grayscale(num_output_channels=3),
        transforms.Resize(size=(512, 512)),
        transforms.RandomRotation(degrees=10),
        transforms.RandomHorizontalFlip(),
        transforms.CenterCrop(size=(224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Grayscale(num_output_channels=3),
        transforms.Resize(size=(512, 512)),
        transforms.CenterCrop(size=(224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Grayscale(num_output_channels=3),
        transforms.Resize(size=(512, 512)),
        transforms.CenterCrop(size=(224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

df = pd.read_csv('/content/drive/MyDrive/training_set_pixel_size_and_HC.csv')
df["img_path"] = '/content/drive/MyDrive/training_set/' + df['filename']

def preprocess_images(image_paths, transform):
    images = []
    for path in tqdm(image_paths, desc='Processing images'):
        img = load_img(path)
        img = transform(img)
        img_array = img_to_array(img)
        img_array = np.transpose(img_array, (1, 2, 0))
        images.append(img_array)
    return np.array(images)

image_paths = [filename for filename in df['img_path']]
preprocessed_images = preprocess_images(image_paths, image_transforms['train'])

preprocessed_images = preprocessed_images / 255.0

X_train, X_val, y_train, y_val = train_test_split(preprocessed_images, np.array(df['head circumference (mm)']), test_size=0.2, random_state=42)

# Define the VGG16 model with the specified input shape and top layer
def vgg16(input_shape, top='flatten'):
    if top not in ('flatten', 'avg', 'max'):
        raise ValueError('unexpected top layer type: %s' % top)
    print('The model is VGG16.')
    base = VGG16(weights='imagenet', input_shape=input_shape, include_top=False)
    x = base.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.7)(x)
    x = Dense(512, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(256, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(128, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(64, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    x = Dense(32, kernel_regularizer=regularizers.l2(0.01),
              activity_regularizer=regularizers.l1(0.01))(x)
    pred = Dense(1, activation='linear')(x)
    model = Model(inputs=base.inputs, outputs=pred)
    return model

model = vgg16((224, 224, 3), top='flatten')

model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error', metrics=['mae'])

checkpoint = ModelCheckpoint('/content/drive/MyDrive/best_model_weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)
early_stopping = EarlyStopping(patience=10, monitor='val_loss', mode='min', verbose=1)

history = model.fit(
    X_train, y_train,
    epochs=100,
    validation_data=(X_val, y_val),
    callbacks=[checkpoint, early_stopping]
)